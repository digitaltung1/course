{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-L0QNBLrKhVE"
   },
   "source": [
    "Choose device out of CPU, GPU, TPU, or Floyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jARBqQIbIlVA"
   },
   "outputs": [],
   "source": [
    "_DEVICE = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pZfX4A_k1cyB",
    "outputId": "ca7f51ee-1cec-4b21-8313-344bb3b39368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.2.0-rc1\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "if _DEVICE == \"TPU\":\n",
    "    device_name = 'tpu'\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "    except ValueError:\n",
    "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "elif _DEVICE == \"GPU\":\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        print (device_name) \n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "elif _DEVICE == \"CPU\":\n",
    "    device_name = '/cpu:0'\n",
    "else:\n",
    "    print (\"Incorrect device option\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6MhmZFU1cx-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow.keras\n",
    "    from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "    from tensorflow.keras.models import Sequential, model_from_json\n",
    "    from tensorflow.keras.preprocessing.image import img_to_array\n",
    "except ImportError as exc:\n",
    "    sys.exit(\"No keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "B6KdzfIl1cx4"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "scXX6GW91cx7"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    sys.exit(\"Cannot import from PIL: Do `pip3 install --user Pillow` to install\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AqIBytu1cyD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as exc:\n",
    "    sys.exit(\"Cannot import scikit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E4o-QLv1cyF"
   },
   "outputs": [],
   "source": [
    "class NetworkConstants():  # pylint: disable=too-few-public-methods\n",
    "    \"\"\"Constant values used as image and network parameters.\"\"\"\n",
    "\n",
    "    # Width of images passed to the network\n",
    "    IMAGE_WIDTH: int = 200\n",
    "\n",
    "    # Height of images passed to the network\n",
    "    IMAGE_HEIGHT: int = 200\n",
    "\n",
    "    # Currently set to 2 alphabet images and 1 background image class\n",
    "    # Number of classes that the network can categorize\n",
    "    NUM_CLASSES: int = 27\n",
    "\n",
    "    # The fraction of images passed to the network during training that should\n",
    "    # be used as a validation set. Range: 0 to 1\n",
    "    VALIDATION_SPLIT: float = 0.2\n",
    "\n",
    "    # The fraction of images passed to the network during training that should\n",
    "    # be used as a test set. Range: 0 to 1\n",
    "    TEST_SPLIT: float = 0.3\n",
    "\n",
    "    # Number of epochs on which to train the network\n",
    "    EPOCHS: int = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "zp3bD_-81cyI"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import io\n",
    "class SignLanguageRecognizer():\n",
    "    \"\"\"Recognize sign language hand signals using Vector's camera feed.\n",
    "\n",
    "    A convolutional neural network is used to predict the hand signs.\n",
    "    The network is built with a Keras Sequential model with a TensorFlow backend.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        device_name = tf.test.gpu_device_name()\n",
    "        if device_name != '/device:GPU:0':\n",
    "            print(\n",
    "               '\\n\\nThis error most likely means that this notebook is not '\n",
    "               'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "               'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "            raise SystemError('GPU device not found')\n",
    "        \"\"\"\n",
    "        self.training_images: np.ndarray = None\n",
    "        self.training_labels: np.ndarray = None\n",
    "        self.test_images: np.ndarray = None\n",
    "        self.test_labels: np.ndarray = None\n",
    "        self.model: tf.keras.engine.sequential.Sequential = None\n",
    "        self.graph: tf.python.framework.ops.Graph = tf.compat.v1.get_default_graph()\n",
    "        self.optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    def load_datasets(self, datasetFile) -> None:\n",
    "        \"\"\"Load the training and test datasets required to train the model.\n",
    "  \n",
    "        \"\"\"\n",
    "\n",
    "        if not dataset:\n",
    "            sys.exit(\"Cannot load dataset.\")\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "        countLabels = dict()\n",
    "        count = 0\n",
    "        with ZipFile(datasetFile) as archive:\n",
    "            for entry in archive.infolist():\n",
    "                filename = entry.filename.split('/')[-1]\n",
    "                if filename.endswith(\".png\") and not filename.startswith(\".\"):\n",
    "                   count += 1\n",
    "                   #if (count % 4 == 0):\n",
    "                   #    continue\n",
    "                   image_data = archive.read(entry.filename)\n",
    "                   data = io.BytesIO(image_data)\n",
    "                   image = Image.open(data)\n",
    "                   if image:    \n",
    "                       # Convert image to an array with shape (image_width, image_height, 1)\n",
    "                       image_data = img_to_array(image)\n",
    "                       images.append(image_data)\n",
    "\n",
    "                       if filename.startswith(\"background\"):\n",
    "                           # Use the last class to denote an unknown/background image\n",
    "                           label = NetworkConstants.NUM_CLASSES - 1\n",
    "                       else:\n",
    "                           # Use ordinal value offsets to denote labels for all alphabets\n",
    "                           label = ord(filename[0]) - 97\n",
    "                       labels.append(label)\n",
    "                       if countLabels.get(chr(label + 97)) is not None:\n",
    "                           countLabels[chr(label + 97)] += 1\n",
    "                       else:\n",
    "                           countLabels[chr(label + 97)] = 1\n",
    "                   else:\n",
    "                       print (\"Not using this file\")\n",
    "                       continue\n",
    "                   \n",
    "\n",
    "        print (\"Here is a count of labels from our repository\")\n",
    "        print (countLabels)\n",
    "        # Normalize the image data\n",
    "        images = np.array(images, dtype=\"float\") / 255.0\n",
    "        # Convert labels to a numpy array\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Split data read in to training and test segments\n",
    "        self.training_images, self.test_images, self.training_labels, self.test_labels = train_test_split(images, labels, \n",
    "                                                                                                          test_size=NetworkConstants.TEST_SPLIT)\n",
    "\n",
    "        # Convert array of labels in to binary classs matrix\n",
    "        self.training_labels = tf.keras.utils.to_categorical(self.training_labels, num_classes=NetworkConstants.NUM_CLASSES)\n",
    "        self.test_labels = tf.keras.utils.to_categorical(self.test_labels, num_classes=NetworkConstants.NUM_CLASSES)\n",
    "    \n",
    "    def create_model(self) -> None:\n",
    "        \"\"\"Creates a convolutional neural network model with the following architecture:\n",
    "\n",
    "        ConvLayer -> MaxPoolLayer -> ConvLayer -> MaxPoolLayer -> ConvLayer ->\n",
    "        Dropout -> Flatten -> Dense -> Dropout -> Dense\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            recognizer = SignLanguageRecognizer()\n",
    "            recognizer.load_datasets(\"/path/to/dataset_root_folder\")\n",
    "            recognizer.create_model()\n",
    "        \"\"\"\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(NetworkConstants.IMAGE_WIDTH, \n",
    "                                                                                      NetworkConstants.IMAGE_HEIGHT, 1)))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "        self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "\n",
    "        self.model.add(Dropout(0.25))\n",
    "        self.model.add(Flatten())\n",
    "\n",
    "        self.model.add(Dense(128, activation=\"relu\"))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(NetworkConstants.NUM_CLASSES, activation=\"softmax\"))\n",
    "\n",
    "        \n",
    "        self.model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                            optimizer=self.optimizer,\n",
    "                            metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "        \n",
    "    def train_model(self, epochs: int = NetworkConstants.EPOCHS, verbosity: int = 1) -> None:\n",
    "        \"\"\"Trains the model off of the training and test data provided\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            recognizer = SignLanguageRecognizer()\n",
    "            recognizer.load_datasets(\"/path/to/dataset_root_folder\")\n",
    "            recognizer.create_model()\n",
    "            recognizer.train_model()\n",
    "        \"\"\"\n",
    "\n",
    "        if self.training_images.size == 0 or self.training_labels.size == 0:\n",
    "            sys.exit(\"Training dataset is empty. Build a dataset with `data_gen.py` before training the model.\")\n",
    "        self.model.fit(self.training_images,\n",
    "                       self.training_labels,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbosity,\n",
    "                       validation_split=NetworkConstants.VALIDATION_SPLIT)\n",
    "      \n",
    "    def load_model(self, model_config_filename: str, model_weights_filename: str) -> None:\n",
    "        \"\"\"Loads a saved model's config and weights to rebuild the model rather than create\n",
    "      tf.keras.optimizers.Adadelta  a new model and re-train.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            recognizer = SignLanguageRecognizer()\n",
    "            recognizer.load_model(\"/path/to/model_config_filename\", \"/path/to/model_weights_filename\")\n",
    "        \"\"\"\n",
    "        if not model_config_filename or not model_weights_filename:\n",
    "            sys.exit(\"Cannot load model. Provide valid paths with --model_config and --model_weights.\")\n",
    "        json_model = None\n",
    "        with open(model_config_filename, \"r\") as file:\n",
    "            json_model = file.read()\n",
    "        # Load the network architecture\n",
    "        self.model = model_from_json(json_model)\n",
    "        # Load the weight information and apply it to the model\n",
    "        self.model.load_weights(model_weights_filename)\n",
    "\n",
    "        self.model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                           optimizer=tf.keras.optimizers.Adadelta,\n",
    "                           metrics=['accuracy'])\n",
    "    def save_model(self, model_config_filename: str, model_weights_filename: str) -> None:\n",
    "        \"\"\"Saves a model's config and weights for latter use.\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            recognizer = SignLanguageRecognizer()\n",
    "            recognizer.load_datasets(args.dataset_root_folder)\n",
    "            recognizer.create_model()\n",
    "            recognizer.train_model()\n",
    "            recognizer.save_model(\"/path/to/model_config_filename\", \"/path/to/model_weights_filename\")\n",
    "        \"\"\"\n",
    "        json_model = self.model.to_json()\n",
    "        # Save the network architecture\n",
    "        with open(model_config_filename, \"w\") as file:\n",
    "            file.write(json_model)\n",
    "        # Save the model's assigned weights\n",
    "        self.model.save_weights(model_weights_filename)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dG8I6zEjY7IZ",
    "outputId": "284f21ba-9aca-4c01-f118-c04013566278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "coQlecBqZIcn",
    "outputId": "f1397b6a-df54-4565-b498-ec713accfd67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.stat_result(st_mode=33152, st_ino=70, st_dev=77, st_nlink=1, st_uid=0, st_gid=0, st_size=133773677, st_atime=1585121187, st_mtime=1585034302, st_ctime=1585034302)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = '/content/drive/My Drive/training-a-robot-to-understand-sign-language.zip'\n",
    "stats = os.stat(dataset)\n",
    "\n",
    "print (stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "AhTlb4BY1cyL",
    "outputId": "c0b78f27-9af7-4685-b16e-dfe088db9451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a count of labels from our repository\n",
      "{'a': 380, 'b': 490, '{': 460, 'c': 380, 'd': 290, 'e': 270, 'f': 340, 'g': 300, 'h': 350, 'i': 350, 'k': 360, 'l': 330, 'm': 610, 'n': 340, 'o': 370, 'p': 260, 'q': 180, 'r': 320, 's': 230, 't': 260, 'u': 329, 'v': 309, 'w': 319, 'x': 279, 'y': 370}\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 198, 198, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 97, 97, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 46, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 135424)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               17334400  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                3483      \n",
      "=================================================================\n",
      "Total params: 17,393,627\n",
      "Trainable params: 17,393,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "149/149 [==============================] - 251s 2s/step - loss: 3.0232 - accuracy: 0.1159 - val_loss: 2.6171 - val_accuracy: 0.2089\n",
      "Epoch 2/20\n",
      "149/149 [==============================] - 249s 2s/step - loss: 2.7043 - accuracy: 0.2172 - val_loss: 2.3574 - val_accuracy: 0.3471\n",
      "Epoch 3/20\n",
      "149/149 [==============================] - 250s 2s/step - loss: 2.5960 - accuracy: 0.2554 - val_loss: 2.3293 - val_accuracy: 0.3218\n",
      "Epoch 4/20\n",
      "149/149 [==============================] - 245s 2s/step - loss: 2.5549 - accuracy: 0.2663 - val_loss: 2.3314 - val_accuracy: 0.3286\n",
      "Epoch 5/20\n",
      "149/149 [==============================] - ETA: 0s - loss: 2.5413 - accuracy: 0.2748"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "recognizer = SignLanguageRecognizer()\n",
    "recognizer.load_datasets(dataset)\n",
    "if _DEVICE == \"TPU\":\n",
    "    with tpu_strategy.scope():\n",
    "        recognizer.create_model()\n",
    "        starttime = timeit.default_timer()\n",
    "        recognizer.train_model()\n",
    "        print(\"The time difference is :\", timeit.default_timer() - starttime)\n",
    "\n",
    "        test_score = recognizer.model.evaluate(recognizer.test_images, recognizer.test_labels, verbose=1)\n",
    "        print(f\"{recognizer.model.metrics_names[1].capitalize()}: {test_score[1] * 100}%\")\n",
    "elif _DEVICE == \"GPU\":\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        recognizer.create_model()\n",
    "        starttime = timeit.default_timer()\n",
    "        recognizer.train_model()\n",
    "        print(\"The time difference is :\", timeit.default_timer() - starttime)\n",
    "        test_score = recognizer.model.evaluate(recognizer.test_images, recognizer.test_labels, verbose=1)\n",
    "        print(f\"{recognizer.model.metrics_names[1].capitalize()}: {test_score[1] * 100}%\")\n",
    "elif _DEVICE == \"CPU\":\n",
    "    with tf.device('/cpu:0'):\n",
    "        recognizer.create_model() \n",
    "        starttime = timeit.default_timer()\n",
    "        recognizer.train_model()\n",
    "        print(\"The time difference is :\", timeit.default_timer() - starttime)\n",
    "\n",
    "        test_score = recognizer.model.evaluate(recognizer.test_images, recognizer.test_labels, verbose=1)\n",
    "        print(f\"{recognizer.model.metrics_names[1].capitalize()}: {test_score[1] * 100}%\")\n",
    "else:\n",
    "    print (\"Nothing to do!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgM8SItB1cyN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kernel-vector-sign-language.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
